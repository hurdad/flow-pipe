namespace:
  create: true
  name: flow-pipe

# --------------------------------------------------
# Etcd (used by API + controller)
# --------------------------------------------------
etcd:
  enabled: true
  image: quay.io/coreos/etcd:v3.5.15
  imagePullPolicy: IfNotPresent
  replicas: 1
  port: 2379
  service:
    # Options: ClusterIP (default), LoadBalancer
    type: ClusterIP
  storage:
    size: 5Gi
    storageClassName: ""
  # Computed into ETCD_ENDPOINTS
  endpoints:
    - http://flow-pipe-etcd:2379

# --------------------------------------------------
# Controller
# --------------------------------------------------
controller:
  image: ghcr.io/hurdad/flow-pipe-controller:main
  imagePullPolicy: IfNotPresent
  replicas: 1
  port: 8080
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 256Mi

# --------------------------------------------------
# API
# --------------------------------------------------
api:
  enabled: true
  image: ghcr.io/hurdad/flow-pipe-api:main
  imagePullPolicy: IfNotPresent
  replicas: 1
  port: 8080

  # These map directly to env vars
  grpcAddr: ":9090"
  httpAddr: ":8080"

  # Used only for container/service ports
  grpcPort: 9090
  httpPort: 8080

  ingress:
    enabled: false
    host: flow-pipe.local
    annotations: {}
    # Optional: specify the ingressClass.
    className: ""
    # Optional: override the path used by the ingress rule.
    path: "/"
    tls:
      enabled: false
      secretName: ""

# --------------------------------------------------
# Observability (optional)
# --------------------------------------------------
observability:
  prometheus:
    enabled: false
    # Optionally point to an existing Prometheus instance instead of installing one.
    endpoint: ""
    server:
      persistentVolume:
        enabled: false

  loki:
    enabled: false
    # Optionally point to an existing Loki instance instead of installing one.
    endpoint: ""
    deploymentMode: SingleBinary
    loki:
      auth_enabled: false

  tempo:
    enabled: false
    # Optionally point to an existing Tempo instance instead of installing one.
    endpoint: ""
    tempo:
      auth_enabled: false
    traces:
      otlp:
        http:
          enabled: true
        grpc:
          enabled: true
    gateway:
      enabled: false
    distributor:
      replicas: 1
    ingester:
      replicas: 1
    compactor:
      replicas: 1

  grafana:
    enabled: false
    # Optionally point to an existing Grafana instance instead of installing one.
    endpoint: ""

  alloy:
    enabled: false
    # Optionally point to an existing Alloy collector instead of installing one.
    endpoint: ""
    # Choose the Alloy workload type: "deployment" for a centralized collector or "daemonset" for per-node collection.
    controller: &alloyController
      type: deployment
    receivers:
      otlp:
        grpcEndpoint: "0.0.0.0:4317"
        httpEndpoint: "0.0.0.0:4318"
    exporters:
      prometheus:
        # Override the Prometheus remote_write endpoint. Defaults to the Prometheus service when enabled.
        endpoint: ""
      loki:
        # Override the Loki push endpoint. Defaults to the Loki service when enabled.
        endpoint: ""
      tempo:
        # Override the Tempo OTLP endpoint. Defaults to the Tempo service when enabled.
        endpoint: ""

grafana:
  adminUser: admin
  adminPassword: admin
  sidecar:
    datasources:
      enabled: true
    dashboards:
      enabled: true
      label: grafana_dashboard

alloy:
  controller: *alloyController
  alloy:
    configMap:
      create: true
      content: |-
        {{- include "flow-pipe.alloy.river" . }}

loki:
  loki:
    commonConfig:
      replication_factor: 1
    schemaConfig:
      configs:
        - from: "2024-04-01"
          store: tsdb
          object_store: s3
          schema: v13
          index:
            prefix: loki_index_
            period: 24h
    ingester:
      chunk_encoding: snappy
    tracing:
      enabled: true
    querier:
      # Default is 4, if you have enough memory and CPU you can increase, reduce if OOMing
      max_concurrent: 2

  #gateway:
  #  ingress:
  #    enabled: true
  #    hosts:
  #      - host: FIXME
  #        paths:
  #          - path: /
  #            pathType: Prefix

  deploymentMode: SingleBinary
  singleBinary:
    replicas: 1
    resources:
      limits:
        cpu: 3
        memory: 4Gi
      requests:
        cpu: 2
        memory: 2Gi
    extraEnv:
      # Keep a little bit lower than memory limits
      - name: GOMEMLIMIT
        value: 3750MiB

  chunksCache:
    # default is 500MB, with limited memory keep this smaller
    writebackSizeLimit: 10MB

  # Enable minio for storage
  minio:
    enabled: true

  # Zero out replica counts of other deployment modes
  backend:
    replicas: 0
  read:
    replicas: 0
  write:
    replicas: 0

  ingester:
    replicas: 0
  querier:
    replicas: 0
  queryFrontend:
    replicas: 0
  queryScheduler:
    replicas: 0
  distributor:
    replicas: 0
  compactor:
    replicas: 0
  indexGateway:
    replicas: 0
  bloomCompactor:
    replicas: 0
  bloomGateway:
    replicas: 0